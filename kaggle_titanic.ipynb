{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "The goal of this model is given set of features to predict if passanger survived the trangedy of RMS Titanic in 1912.\n",
    "\n",
    "I have done no research on other participant contributions to not get biased towards their solutions and try something completely of my own.\n",
    "\n",
    "## Data ##\n",
    "**Dataset:** 891 data points, 10 features, 1 label\n",
    "**Test set:** 418 data points, 10 features\n",
    "\n",
    "**Features**:\n",
    "- Class of travel\n",
    "- Name\n",
    "- Gender\n",
    "- Age\n",
    "- Number of Sibling/Spouse aboard\n",
    "- Number of Parent/Child aboard\n",
    "- Ticket\n",
    "- Fare\n",
    "- Cabin\n",
    "- Embarked\n",
    "\n",
    "Not all passangers have all features. Also some features such as Name, Ticket or Cabin do not have standardized format.\n",
    "\n",
    "**Labels**:\n",
    "- Survived: 1 if passanger survived the tragedy, 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "if tf.__version__ == \"2.0.0-alpha0\":\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras import optimizers\n",
    "    from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "    from tensorflow.keras.models import Model\n",
    "else:\n",
    "    from keras import layers\n",
    "    from keras import optimizers\n",
    "    from keras.layers import Input, Dense, Flatten\n",
    "    from keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "print(tf.__version__)\n",
    "\n",
    "random.seed(945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 891\n",
      "#features: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"#samples: \"+str(dataset.shape[0]))\n",
    "print(\"#features: \"+str(dataset.shape[1]-2)) # -2 for Id and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features might not be present in all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    891\n",
      "Survived       891\n",
      "Pclass         891\n",
      "Name           891\n",
      "Sex            891\n",
      "Age            714\n",
      "SibSp          891\n",
      "Parch          891\n",
      "Ticket         891\n",
      "Fare           891\n",
      "Cabin          204\n",
      "Embarked       889\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passangers that survived: 342, 38.38%\n",
      "Passangers that did not survived: 549, 61.62%\n"
     ]
    }
   ],
   "source": [
    "num_of_survived = dataset[dataset.Survived == 1].Survived.count()\n",
    "num_of_not_survived = dataset.shape[0]-num_of_survived\n",
    "print(\"Passangers that survived: %d, %.2f%%\" % (num_of_survived,  num_of_survived/dataset.shape[0]*100))\n",
    "print(\"Passangers that did not survived: %d, %.2f%%\" % (num_of_not_survived,  num_of_not_survived/dataset.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis ##\n",
    "\n",
    "For the first, simplest model, lets focus on features that do not need a lot of preprocessing. Thus we ommit Name, Ticket and Cabin.\n",
    "\n",
    "Name feature could give us information about wealth of the passanger, however we already receive the Class of travel which should be good indicator and we might need much bigger dataset to extract such information from the dataset.\n",
    "\n",
    "Cabin feature could be very helpful, however Cabin information is present in only 204 samples, is unique (would need preprocessing to add information about the location of the cabin) and some passangers have more than 1.\n",
    "\n",
    "Ticket feature might give similar information as Cabin and Class of travel, but similarly as in Cabin needs research to extract useful information and preprocessing since it does not always have standardized format.\n",
    "\n",
    "**Class of travel**\n",
    "\n",
    "Contains information about wealth of the passanger and likely position of the cabin on the ship.\n",
    "\n",
    "It can take value from 1,2 or 3 from 1 being the most luxurious. Lets see the distribution in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived  Pclass\n",
       "0         1          80\n",
       "          2          97\n",
       "          3         372\n",
       "1         1         136\n",
       "          2          87\n",
       "          3         119\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['Survived', 'Pclass'])[\"Pclass\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From numbers above it can be seen that only 1/4 of passangers in 3rd class in the dataset survived, however more than 3/5 of passangers in 1st class survived. Showing that Class of travel is possibly good feature.\n",
    "\n",
    "**Gender**\n",
    "\n",
    "Gender is potentially very usefull feature considering RMS Titanic being famous for the phrase \"Woman and children first\". According to blog post (https://blog.minitab.com/blog/real-world-quality-improvement/analyzing-titanic-survival-rates) which analyzed available data 73% of female passangers survived and only 21% of male passangers did.\n",
    "\n",
    "Lets see the distribution in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived  Sex   \n",
       "0         female     81\n",
       "          male      468\n",
       "1         female    233\n",
       "          male      109\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['Survived', 'Sex'])[\"Sex\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female survived: 74.20%, Male survived: 18.89%\n"
     ]
    }
   ],
   "source": [
    "print(\"Female survived: %.2f%%, Male survived: %.2f%%\" % (233/(81+233)*100, 109/(109+468)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "I suspect Age to be partially missledaing. Firstly, I lack knowledge about the policy that took place considering age - if they prioritized elderly or young. However even if prioritization would go towards elderly, they might be more prone to not being able to get to lifeboats in time.\n",
    "\n",
    "Again, lets look at the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af2100f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"Survived\"]==0][\"Age\"].hist(bins=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af432a160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Survived\"]==1][\"Age\"].hist(bins=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Distributions seem very similar, however one can notice significantly higher group of younger passangers survived and signifantly higher group of older passangers did not. This suggests children were priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Sibling/Spouse aboard and Number of Parent/Child aboard**\n",
    "\n",
    "These features might not seem valuable at first, however if it is true that male passangers were let on the lifeboats if they had children or maybe with their spouse/whole family. Lets look at few histograms that might give us some idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af4378048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Survived\"]==0][\"SibSp\"].hist(bins=10)\n",
    "dataset[dataset[\"Survived\"]==1][\"SibSp\"].hist(bins=5, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af4481f98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Survived\"]==0][\"Parch\"].hist(bins=9)\n",
    "dataset[dataset[\"Survived\"]==1][\"Parch\"].hist(bins=8, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[(dataset[\"Survived\"]==0) & (dataset[\"SibSp\"]>0) & (dataset[\"Parch\"]>0)][\"Survived\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[(dataset[\"Survived\"]==1) & (dataset[\"SibSp\"]>0) & (dataset[\"Parch\"]>0)][\"Survived\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a lot of siblings/spouses and children/parents aboard (>4) seems to be bad for your survival chances. It makes sense since you are more likely to refuse board lifeboat when your family is not together or you are not all allowed to board the lifeboat together.\n",
    "\n",
    "Idea that having more than 0 from both categories, thus traveling with family, might be positive seems to be false since more of those passangers died than survived.\n",
    "\n",
    "**Embarked**\n",
    "\n",
    "The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown. My assumption is this feature has no influence. If the analysis confirms the distribution is close to uniform I shall ommit this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived  Embarked\n",
       "0         C            75\n",
       "          Q            47\n",
       "          S           427\n",
       "1         C            93\n",
       "          Q            30\n",
       "          S           217\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['Survived', 'Embarked'])[\"Embarked\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived - S:0.34% C:0.55% Q:0.39%\n"
     ]
    }
   ],
   "source": [
    "print(\"Survived - S:%.2f%% C:%.2f%% Q:%.2f%%\" % (217/(427+217),93/(75+93),30/(30+47)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked  Pclass\n",
       "C         1          85\n",
       "          2          17\n",
       "          3          66\n",
       "Q         1           2\n",
       "          2           3\n",
       "          3          72\n",
       "S         1         127\n",
       "          2         164\n",
       "          3         353\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['Embarked', 'Pclass'])[\"Pclass\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that passangers traveling from Cherbough which was 2nd out of 3 boarding places had higher chance of survival than passangers boarding on the other. This correlates with Pclass since there is high percentage of passangers from 1st class that boarded on this stop. I will keep it in the dataset but might try to ommit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fare**\n",
    "\n",
    "Fare feature will most likely be very correlated with Pclass. Lets see if that is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af44f6320>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Pclass\"]==3][\"Fare\"].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af454ceb8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Pclass\"]==2][\"Fare\"].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20af55dfcf8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[dataset[\"Pclass\"]==1][\"Fare\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even tough the there are some passangers that payed similar Fare in 3rd and 1st class, there seems to be visible correlation.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "I think Pclass, Sex and Age are the features that give us the most information. We will try few classifiers with different set of features, however these 3 should always be present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing ##\n",
    "\n",
    "Lets select only features we decided to use now and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red = dataset[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\", \"Embarked\"]]\n",
    "mapping_sex = {'male': 0, 'female': 1}\n",
    "mapping_embarked = {'S': 0, 'C': 1, 'Q': 2}\n",
    "dataset_red = dataset_red.applymap(lambda s: mapping_sex.get(s) if s in mapping_sex else s)\n",
    "dataset_red = dataset_red.applymap(lambda s: mapping_embarked.get(s) if s in mapping_embarked else s)\n",
    "dataset_red_norm = dataset_red.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "dataset_red_norm[\"Survived\"] = dataset_red[\"Survived\"]\n",
    "dataset_red_norm_no_nan = dataset_red_norm.fillna(value='0')\n",
    "dataset_red_norm[\"Age\"] = pd.to_numeric(dataset_red_norm[\"Age\"], errors='ignore')\n",
    "dataset_red_norm[\"Embarked\"] = pd.to_numeric(dataset_red_norm[\"Age\"], errors='ignore')\n",
    "dataset_red_norm_no_nan[\"Age\"] = pd.to_numeric(dataset_red_norm_no_nan[\"Age\"], errors='ignore')\n",
    "dataset_red_norm_no_nan[\"Embarked\"] = pd.to_numeric(dataset_red_norm_no_nan[\"Age\"], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_red_norm_no_nan, test_size=0.2, random_state = 101)\n",
    "y_train = train[\"Survived\"]\n",
    "x_train = train.loc[:, train.columns != 'Survived']\n",
    "y_test = test[\"Survived\"]\n",
    "x_test = test.loc[:, test.columns != 'Survived']\n",
    "\n",
    "y_trainNaN = train[\"Survived\"]\n",
    "x_trainNaN = train.loc[:, train.columns != 'Survived']\n",
    "y_testNaN = test[\"Survived\"]\n",
    "x_testNaN = test.loc[:, test.columns != 'Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classical classifiers ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=293, solver='lbfgs', tol=1e-10, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf = LogisticRegression(max_iter=100, penalty='l2', solver=\"lbfgs\", tol=1e-10, random_state=293)\n",
    "logreg_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7991573033707865"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932960893854749"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf.predict(x_test.iloc[range(20),:]) == y_test.iloc[range(20)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(gamma='auto', kernel='rbf', C=1000, random_state=934)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=934, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8342696629213483"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268156424581006"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True, False,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict(x_test.iloc[range(20),:]) == y_test.iloc[range(20)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters - depth:4 - num_est:20 - lr:0.22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=4,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.2154, n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_depth = None\n",
    "#best_num_clas = None\n",
    "#best_lr = None\n",
    "#best_test_score = 0\n",
    "#\n",
    "#counter = 0\n",
    "#for depth in range(2,10):\n",
    "#    for est_num in np.logspace(1,2.9,20):\n",
    "#        est_num = int(np.round(est_num))\n",
    "#        for lr in np.logspace(-3,1,25):\n",
    "#            ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=depth), n_estimators=est_num, learning_rate=lr, random_state=0)\n",
    "#            ada_clf.fit(x_train,y_train)\n",
    "#            curr_score = ada_clf.score(x_test, y_test)\n",
    "#            counter += 1\n",
    "#            if counter%20==0:\n",
    "#                print(\"Working... iteration: %i\" % (counter))\n",
    "#            if curr_score > best_test_score:\n",
    "#                best_test_score = curr_score\n",
    "#                best_depth = depth\n",
    "#                best_num_clas = est_num\n",
    "#                best_lr = lr\n",
    "#                print(\"Best Found new best parameters: - depth:%d - num_est:%d - lr:%.4f\\n Best test score:%.4f\" % (best_depth, best_num_clas, best_lr, curr_score))\n",
    "\n",
    "best_depth=4\n",
    "best_num_clas=20\n",
    "best_lr=0.2154\n",
    "print(\"Best parameters - depth:%d - num_est:%d - lr:%.2f\" % (best_depth, best_num_clas, best_lr))\n",
    "ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=best_depth), n_estimators=best_num_clas, learning_rate=best_lr, random_state=0)\n",
    "ada_clf.fit(x_train,y_train)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9367977528089888"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547486033519553"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.predict(x_test.iloc[range(20,50),:]) == y_test.iloc[range(20,50)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"dataset/test.csv\")\n",
    "test_data_red = test_data[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\", \"Embarked\"]]\n",
    "mapping_sex = {'male': 0, 'female': 1}\n",
    "mapping_embarked = {'S': 0, 'C': 1, 'Q': 2}\n",
    "test_data_red = test_data_red.applymap(lambda s: mapping_sex.get(s) if s in mapping_sex else s)\n",
    "test_data_red = test_data_red.applymap(lambda s: mapping_embarked.get(s) if s in mapping_embarked else s)\n",
    "test_data_red_norm = test_data_red.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "test_data_red_norm = test_data_red_norm.fillna(value='0')\n",
    "\n",
    "selected_clf = ada_clf\n",
    "predictions = ada_clf.predict(test_data_red_norm)\n",
    "test_predictions = test_data[[\"PassengerId\"]]\n",
    "test_predictions.insert(loc=1, column='Survived', value=predictions)\n",
    "test_predictions.to_csv(\"kaggle_submission_adaboost\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      float64\n",
       "Sex         float64\n",
       "Age         float64\n",
       "SibSp       float64\n",
       "Parch       float64\n",
       "Fare        float64\n",
       "Embarked    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trainNaN.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9185393258426966, Test score: 0.8659217877094972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_depth = None\n",
    "#best_num_clas = None\n",
    "#best_lr = None\n",
    "#best_test_score = 0\n",
    "#\n",
    "#counter = 0\n",
    "#for depth in range(2,5):\n",
    "#    for est_num in np.logspace(1.7,1.89,20):\n",
    "#        est_num = int(np.round(est_num))\n",
    "#        for lr in np.logspace(-0.5,-0.1,25):\n",
    "#            xgboost_clf = XGBClassifier(max_depth=depth, learning_rate=lr, n_estimators=est_num, missing=np.nan, seed=92)\n",
    "#            xgboost_clf.fit(x_trainNaN,y_trainNaN)\n",
    "#            curr_score = xgboost_clf.score(x_testNaN, y_testNaN)\n",
    "#            counter += 1\n",
    "#            if counter%20==0:\n",
    "#                print(\"Working... iteration: %i\" % (counter))\n",
    "#            if curr_score > best_test_score:\n",
    "#                best_test_score = curr_score\n",
    "#                best_depth = depth\n",
    "#                best_num_clas = est_num\n",
    "#                best_lr = lr\n",
    "#                print(\"Best Found new best parameters: - depth:%d - num_est:%d - lr:%.4f\\n Best test score:%.4f\" % (best_depth, best_num_clas, best_lr, curr_score))\n",
    "xgboost_clf = XGBClassifier(max_depth=3, learning_rate=0.4137, n_estimators=76, missing=np.nan, seed=92)\n",
    "xgboost_clf.fit(x_trainNaN,y_trainNaN)\n",
    "print(\"Train score: {}, Test score: {}\".format(xgboost_clf.score(x_trainNaN, y_trainNaN),xgboost_clf.score(x_testNaN, y_testNaN)))\n",
    "xgboost_clf.predict(x_test.iloc[range(20,50),:]) == y_test.iloc[range(20,50)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"dataset/test.csv\")\n",
    "test_data_red = test_data[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\", \"Embarked\"]]\n",
    "mapping_sex = {'male': 0, 'female': 1}\n",
    "mapping_embarked = {'S': 0, 'C': 1, 'Q': 2}\n",
    "test_data_red = test_data_red.applymap(lambda s: mapping_sex.get(s) if s in mapping_sex else s)\n",
    "test_data_red = test_data_red.applymap(lambda s: mapping_embarked.get(s) if s in mapping_embarked else s)\n",
    "test_data_red_norm = test_data_red.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "selected_clf = xgboost_clf\n",
    "predictions = xgboost_clf.predict(test_data_red_norm)\n",
    "test_predictions = test_data[[\"PassengerId\"]]\n",
    "test_predictions.insert(loc=1, column='Survived', value=predictions)\n",
    "test_predictions.to_csv(\"kaggle_submission_xgboost\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training simple NNs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TitanicModel(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(256, activation='relu', name='dense1')(X)\n",
    "    X = Dense(128, activation='relu', name='dense2')(X)\n",
    "    X = Dense(128, activation='relu', name='dense3')(X)\n",
    "    X = Dense(64, activation='relu', name='dense4')(X)\n",
    "    X = Dense(32, activation='relu', name='dense5')(X)\n",
    "    X = Dense(1, activation='sigmoid', name='dense6')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='TitanicModel')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TitanicModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1, 7)]            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense5 (Dense)               (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 61,825\n",
      "Trainable params: 61,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "titanicModel = TitanicModel((1,7))\n",
    "adamOpt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=10**(-8))\n",
    "titanicModel.compile(optimizer = adamOpt, loss = \"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "titanicModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_keras = x_train.values.reshape(712,1,7)\n",
    "y_train_keras = y_train.values.reshape(712,1)\n",
    "x_test_keras = x_test.values.reshape(179,1,7)\n",
    "y_test_keras = y_test.values.reshape(179,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "712/712 [==============================] - 0s 214us/sample - loss: 0.2472 - accuracy: 0.7626\n",
      "Epoch 2/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.2428 - accuracy: 0.7907\n",
      "Epoch 3/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.2384 - accuracy: 0.7963\n",
      "Epoch 4/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.2335 - accuracy: 0.7921\n",
      "Epoch 5/500\n",
      "712/712 [==============================] - 0s 30us/sample - loss: 0.2275 - accuracy: 0.7893\n",
      "Epoch 6/500\n",
      "712/712 [==============================] - 0s 33us/sample - loss: 0.2195 - accuracy: 0.7921\n",
      "Epoch 7/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.2091 - accuracy: 0.7879\n",
      "Epoch 8/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1968 - accuracy: 0.7907\n",
      "Epoch 9/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1834 - accuracy: 0.7949\n",
      "Epoch 10/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1710 - accuracy: 0.7949\n",
      "Epoch 11/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1609 - accuracy: 0.8062\n",
      "Epoch 12/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1528 - accuracy: 0.8048\n",
      "Epoch 13/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1478 - accuracy: 0.8118\n",
      "Epoch 14/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1439 - accuracy: 0.8132\n",
      "Epoch 15/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1412 - accuracy: 0.8090\n",
      "Epoch 16/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1389 - accuracy: 0.8118\n",
      "Epoch 17/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1373 - accuracy: 0.8104\n",
      "Epoch 18/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1359 - accuracy: 0.8076\n",
      "Epoch 19/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1351 - accuracy: 0.8118\n",
      "Epoch 20/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1344 - accuracy: 0.8230\n",
      "Epoch 21/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1342 - accuracy: 0.8202\n",
      "Epoch 22/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1329 - accuracy: 0.8146\n",
      "Epoch 23/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1323 - accuracy: 0.8202\n",
      "Epoch 24/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1320 - accuracy: 0.8160\n",
      "Epoch 25/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1317 - accuracy: 0.8258\n",
      "Epoch 26/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1309 - accuracy: 0.8258\n",
      "Epoch 27/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1307 - accuracy: 0.8216\n",
      "Epoch 28/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1307 - accuracy: 0.8230\n",
      "Epoch 29/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1301 - accuracy: 0.8258\n",
      "Epoch 30/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1295 - accuracy: 0.8230\n",
      "Epoch 31/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1296 - accuracy: 0.8244\n",
      "Epoch 32/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1291 - accuracy: 0.8230\n",
      "Epoch 33/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1287 - accuracy: 0.8230\n",
      "Epoch 34/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1283 - accuracy: 0.8272\n",
      "Epoch 35/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1281 - accuracy: 0.8244\n",
      "Epoch 36/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1282 - accuracy: 0.8272\n",
      "Epoch 37/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1283 - accuracy: 0.8301\n",
      "Epoch 38/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1275 - accuracy: 0.8272\n",
      "Epoch 39/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1273 - accuracy: 0.8258\n",
      "Epoch 40/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1270 - accuracy: 0.8272\n",
      "Epoch 41/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1271 - accuracy: 0.8287\n",
      "Epoch 42/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1273 - accuracy: 0.8230\n",
      "Epoch 43/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1260 - accuracy: 0.8301\n",
      "Epoch 44/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1272 - accuracy: 0.8258\n",
      "Epoch 45/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1263 - accuracy: 0.8357\n",
      "Epoch 46/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1265 - accuracy: 0.8315\n",
      "Epoch 47/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1257 - accuracy: 0.8315\n",
      "Epoch 48/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1253 - accuracy: 0.8315\n",
      "Epoch 49/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1252 - accuracy: 0.8272\n",
      "Epoch 50/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1250 - accuracy: 0.8301\n",
      "Epoch 51/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1253 - accuracy: 0.8315\n",
      "Epoch 52/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1248 - accuracy: 0.8329\n",
      "Epoch 53/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1245 - accuracy: 0.8357\n",
      "Epoch 54/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1243 - accuracy: 0.8357\n",
      "Epoch 55/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1238 - accuracy: 0.8343\n",
      "Epoch 56/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1239 - accuracy: 0.8357\n",
      "Epoch 57/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1240 - accuracy: 0.8399\n",
      "Epoch 58/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1239 - accuracy: 0.8399\n",
      "Epoch 59/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1238 - accuracy: 0.8343\n",
      "Epoch 60/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1239 - accuracy: 0.8329\n",
      "Epoch 61/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1240 - accuracy: 0.8315\n",
      "Epoch 62/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1233 - accuracy: 0.8357\n",
      "Epoch 63/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1235 - accuracy: 0.8357\n",
      "Epoch 64/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1230 - accuracy: 0.8371\n",
      "Epoch 65/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1232 - accuracy: 0.8371\n",
      "Epoch 66/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1230 - accuracy: 0.8343\n",
      "Epoch 67/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1237 - accuracy: 0.8315\n",
      "Epoch 68/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1224 - accuracy: 0.8371\n",
      "Epoch 69/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1226 - accuracy: 0.8399\n",
      "Epoch 70/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1227 - accuracy: 0.8343\n",
      "Epoch 71/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1232 - accuracy: 0.8315\n",
      "Epoch 72/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1224 - accuracy: 0.8301\n",
      "Epoch 73/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1220 - accuracy: 0.8357\n",
      "Epoch 74/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1223 - accuracy: 0.8357\n",
      "Epoch 75/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1221 - accuracy: 0.8371\n",
      "Epoch 76/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1221 - accuracy: 0.8385\n",
      "Epoch 77/500\n",
      "712/712 [==============================] - 0s 38us/sample - loss: 0.1221 - accuracy: 0.8399\n",
      "Epoch 78/500\n",
      "712/712 [==============================] - 0s 38us/sample - loss: 0.1222 - accuracy: 0.8399\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1217 - accuracy: 0.8399\n",
      "Epoch 80/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1214 - accuracy: 0.8413\n",
      "Epoch 81/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1218 - accuracy: 0.8413\n",
      "Epoch 82/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1217 - accuracy: 0.8371\n",
      "Epoch 83/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1219 - accuracy: 0.8357\n",
      "Epoch 84/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1213 - accuracy: 0.8343\n",
      "Epoch 85/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1213 - accuracy: 0.8399\n",
      "Epoch 86/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1211 - accuracy: 0.8385\n",
      "Epoch 87/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1209 - accuracy: 0.8399\n",
      "Epoch 88/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1210 - accuracy: 0.8371\n",
      "Epoch 89/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1212 - accuracy: 0.8385\n",
      "Epoch 90/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1212 - accuracy: 0.8371\n",
      "Epoch 91/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1215 - accuracy: 0.8385\n",
      "Epoch 92/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1211 - accuracy: 0.8385\n",
      "Epoch 93/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1206 - accuracy: 0.8385\n",
      "Epoch 94/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1209 - accuracy: 0.8399\n",
      "Epoch 95/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1205 - accuracy: 0.8385\n",
      "Epoch 96/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1202 - accuracy: 0.8441\n",
      "Epoch 97/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1204 - accuracy: 0.8427\n",
      "Epoch 98/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1206 - accuracy: 0.8399\n",
      "Epoch 99/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1207 - accuracy: 0.8385\n",
      "Epoch 100/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1205 - accuracy: 0.8441\n",
      "Epoch 101/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1199 - accuracy: 0.8441\n",
      "Epoch 102/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1208 - accuracy: 0.8385\n",
      "Epoch 103/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1202 - accuracy: 0.8385\n",
      "Epoch 104/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1204 - accuracy: 0.8399\n",
      "Epoch 105/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1199 - accuracy: 0.8427\n",
      "Epoch 106/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1198 - accuracy: 0.8399\n",
      "Epoch 107/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1206 - accuracy: 0.8371\n",
      "Epoch 108/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1217 - accuracy: 0.8371\n",
      "Epoch 109/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1206 - accuracy: 0.8371\n",
      "Epoch 110/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1201 - accuracy: 0.8413\n",
      "Epoch 111/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1199 - accuracy: 0.8427\n",
      "Epoch 112/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1198 - accuracy: 0.8413\n",
      "Epoch 113/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1195 - accuracy: 0.8413\n",
      "Epoch 114/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1198 - accuracy: 0.8413\n",
      "Epoch 115/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1195 - accuracy: 0.8441\n",
      "Epoch 116/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1197 - accuracy: 0.8385\n",
      "Epoch 117/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1197 - accuracy: 0.8399\n",
      "Epoch 118/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1193 - accuracy: 0.8399\n",
      "Epoch 119/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1204 - accuracy: 0.8427\n",
      "Epoch 120/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1194 - accuracy: 0.8427\n",
      "Epoch 121/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1192 - accuracy: 0.8427\n",
      "Epoch 122/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1187 - accuracy: 0.8413\n",
      "Epoch 123/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1197 - accuracy: 0.8399\n",
      "Epoch 124/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1190 - accuracy: 0.8399\n",
      "Epoch 125/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1216 - accuracy: 0.8357\n",
      "Epoch 126/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1210 - accuracy: 0.8357\n",
      "Epoch 127/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1194 - accuracy: 0.8399\n",
      "Epoch 128/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1194 - accuracy: 0.8385\n",
      "Epoch 129/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1189 - accuracy: 0.8441\n",
      "Epoch 130/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1188 - accuracy: 0.8427\n",
      "Epoch 131/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1189 - accuracy: 0.8441\n",
      "Epoch 132/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1189 - accuracy: 0.8427\n",
      "Epoch 133/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1188 - accuracy: 0.8441\n",
      "Epoch 134/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1194 - accuracy: 0.8385\n",
      "Epoch 135/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1189 - accuracy: 0.8399\n",
      "Epoch 136/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1184 - accuracy: 0.8413\n",
      "Epoch 137/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1183 - accuracy: 0.8427\n",
      "Epoch 138/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1184 - accuracy: 0.8413\n",
      "Epoch 139/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1184 - accuracy: 0.8455\n",
      "Epoch 140/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1184 - accuracy: 0.8441\n",
      "Epoch 141/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1184 - accuracy: 0.8413\n",
      "Epoch 142/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1201 - accuracy: 0.8441\n",
      "Epoch 143/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1185 - accuracy: 0.8371\n",
      "Epoch 144/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1190 - accuracy: 0.8371\n",
      "Epoch 145/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1182 - accuracy: 0.8441\n",
      "Epoch 146/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1183 - accuracy: 0.8413\n",
      "Epoch 147/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1181 - accuracy: 0.8427\n",
      "Epoch 148/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1182 - accuracy: 0.8441\n",
      "Epoch 149/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1181 - accuracy: 0.8413\n",
      "Epoch 150/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1185 - accuracy: 0.8441\n",
      "Epoch 151/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1177 - accuracy: 0.8413\n",
      "Epoch 152/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1180 - accuracy: 0.8413\n",
      "Epoch 153/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1186 - accuracy: 0.8413\n",
      "Epoch 154/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1180 - accuracy: 0.8441\n",
      "Epoch 155/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1186 - accuracy: 0.8399\n",
      "Epoch 156/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1176 - accuracy: 0.8427\n",
      "Epoch 157/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1180 - accuracy: 0.8427\n",
      "Epoch 158/500\n",
      "712/712 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.81 - 0s 32us/sample - loss: 0.1176 - accuracy: 0.8455\n",
      "Epoch 159/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1173 - accuracy: 0.8427\n",
      "Epoch 160/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1180 - accuracy: 0.8427\n",
      "Epoch 161/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1180 - accuracy: 0.8455\n",
      "Epoch 162/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1171 - accuracy: 0.8413\n",
      "Epoch 163/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1189 - accuracy: 0.8455\n",
      "Epoch 164/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1169 - accuracy: 0.8441\n",
      "Epoch 165/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1174 - accuracy: 0.8413\n",
      "Epoch 166/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1171 - accuracy: 0.8413\n",
      "Epoch 167/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1174 - accuracy: 0.8441\n",
      "Epoch 168/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1170 - accuracy: 0.8413\n",
      "Epoch 169/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1170 - accuracy: 0.8455\n",
      "Epoch 170/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1174 - accuracy: 0.8413\n",
      "Epoch 171/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1170 - accuracy: 0.8455\n",
      "Epoch 172/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1168 - accuracy: 0.8469\n",
      "Epoch 173/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1170 - accuracy: 0.8413\n",
      "Epoch 174/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1170 - accuracy: 0.8441\n",
      "Epoch 175/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1174 - accuracy: 0.8455\n",
      "Epoch 176/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1165 - accuracy: 0.8469\n",
      "Epoch 177/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1167 - accuracy: 0.8441\n",
      "Epoch 178/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1168 - accuracy: 0.8427\n",
      "Epoch 179/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1173 - accuracy: 0.8413\n",
      "Epoch 180/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1172 - accuracy: 0.8455\n",
      "Epoch 181/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1169 - accuracy: 0.8441\n",
      "Epoch 182/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1172 - accuracy: 0.8441\n",
      "Epoch 183/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1165 - accuracy: 0.8469\n",
      "Epoch 184/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1184 - accuracy: 0.8371\n",
      "Epoch 185/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1163 - accuracy: 0.8427\n",
      "Epoch 186/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1175 - accuracy: 0.8455\n",
      "Epoch 187/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1172 - accuracy: 0.8413\n",
      "Epoch 188/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1178 - accuracy: 0.8413\n",
      "Epoch 189/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1163 - accuracy: 0.8441\n",
      "Epoch 190/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1166 - accuracy: 0.8441\n",
      "Epoch 191/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1163 - accuracy: 0.8441\n",
      "Epoch 192/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1164 - accuracy: 0.8427\n",
      "Epoch 193/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1159 - accuracy: 0.8399\n",
      "Epoch 194/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1164 - accuracy: 0.8455\n",
      "Epoch 195/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1167 - accuracy: 0.8441\n",
      "Epoch 196/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1160 - accuracy: 0.8469\n",
      "Epoch 197/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1166 - accuracy: 0.8469\n",
      "Epoch 198/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1160 - accuracy: 0.8455\n",
      "Epoch 199/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1165 - accuracy: 0.8455\n",
      "Epoch 200/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1163 - accuracy: 0.8413\n",
      "Epoch 201/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1162 - accuracy: 0.8427\n",
      "Epoch 202/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1158 - accuracy: 0.8455\n",
      "Epoch 203/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1163 - accuracy: 0.8413\n",
      "Epoch 204/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1161 - accuracy: 0.8427\n",
      "Epoch 205/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1161 - accuracy: 0.8427\n",
      "Epoch 206/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1154 - accuracy: 0.8469\n",
      "Epoch 207/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1157 - accuracy: 0.8455\n",
      "Epoch 208/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1154 - accuracy: 0.8441\n",
      "Epoch 209/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1161 - accuracy: 0.8469\n",
      "Epoch 210/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1152 - accuracy: 0.8441\n",
      "Epoch 211/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1159 - accuracy: 0.8441\n",
      "Epoch 212/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1155 - accuracy: 0.8441\n",
      "Epoch 213/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1154 - accuracy: 0.8483\n",
      "Epoch 214/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1162 - accuracy: 0.8469\n",
      "Epoch 215/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1164 - accuracy: 0.8441\n",
      "Epoch 216/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1159 - accuracy: 0.8441\n",
      "Epoch 217/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1154 - accuracy: 0.8441\n",
      "Epoch 218/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1154 - accuracy: 0.8455\n",
      "Epoch 219/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1150 - accuracy: 0.8441\n",
      "Epoch 220/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1158 - accuracy: 0.8483\n",
      "Epoch 221/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1155 - accuracy: 0.8483\n",
      "Epoch 222/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1155 - accuracy: 0.8455\n",
      "Epoch 223/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1157 - accuracy: 0.8441\n",
      "Epoch 224/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1150 - accuracy: 0.8455\n",
      "Epoch 225/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1158 - accuracy: 0.8497\n",
      "Epoch 226/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1151 - accuracy: 0.8483\n",
      "Epoch 227/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1150 - accuracy: 0.8441\n",
      "Epoch 228/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1165 - accuracy: 0.8413\n",
      "Epoch 229/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1149 - accuracy: 0.8469\n",
      "Epoch 230/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1153 - accuracy: 0.8455\n",
      "Epoch 231/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1154 - accuracy: 0.8427\n",
      "Epoch 232/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1153 - accuracy: 0.8497\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1150 - accuracy: 0.8427\n",
      "Epoch 234/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1159 - accuracy: 0.8413\n",
      "Epoch 235/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1163 - accuracy: 0.8427\n",
      "Epoch 236/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1148 - accuracy: 0.8497\n",
      "Epoch 237/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1147 - accuracy: 0.8441\n",
      "Epoch 238/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1150 - accuracy: 0.8483\n",
      "Epoch 239/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1152 - accuracy: 0.8455\n",
      "Epoch 240/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1141 - accuracy: 0.8497\n",
      "Epoch 241/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1144 - accuracy: 0.8497\n",
      "Epoch 242/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1148 - accuracy: 0.8497\n",
      "Epoch 243/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1142 - accuracy: 0.8525\n",
      "Epoch 244/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1143 - accuracy: 0.8469\n",
      "Epoch 245/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1148 - accuracy: 0.8455\n",
      "Epoch 246/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1160 - accuracy: 0.8413\n",
      "Epoch 247/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1144 - accuracy: 0.8511\n",
      "Epoch 248/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1146 - accuracy: 0.8483\n",
      "Epoch 249/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1146 - accuracy: 0.8497\n",
      "Epoch 250/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1140 - accuracy: 0.8497\n",
      "Epoch 251/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1143 - accuracy: 0.8483\n",
      "Epoch 252/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1152 - accuracy: 0.8469\n",
      "Epoch 253/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1154 - accuracy: 0.8511\n",
      "Epoch 254/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1144 - accuracy: 0.8511\n",
      "Epoch 255/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1141 - accuracy: 0.8469\n",
      "Epoch 256/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1138 - accuracy: 0.8497\n",
      "Epoch 257/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1138 - accuracy: 0.8511\n",
      "Epoch 258/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1140 - accuracy: 0.8511\n",
      "Epoch 259/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1136 - accuracy: 0.8497\n",
      "Epoch 260/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1189 - accuracy: 0.8399\n",
      "Epoch 261/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1151 - accuracy: 0.8469\n",
      "Epoch 262/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1156 - accuracy: 0.8427\n",
      "Epoch 263/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1147 - accuracy: 0.8497\n",
      "Epoch 264/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1147 - accuracy: 0.8441\n",
      "Epoch 265/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1149 - accuracy: 0.8483\n",
      "Epoch 266/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1142 - accuracy: 0.8497\n",
      "Epoch 267/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1140 - accuracy: 0.8497\n",
      "Epoch 268/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1135 - accuracy: 0.8469\n",
      "Epoch 269/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1150 - accuracy: 0.8441\n",
      "Epoch 270/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1139 - accuracy: 0.8497\n",
      "Epoch 271/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1134 - accuracy: 0.8511\n",
      "Epoch 272/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1146 - accuracy: 0.8497\n",
      "Epoch 273/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1138 - accuracy: 0.8497\n",
      "Epoch 274/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1138 - accuracy: 0.8511\n",
      "Epoch 275/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1136 - accuracy: 0.8525\n",
      "Epoch 276/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1138 - accuracy: 0.8511\n",
      "Epoch 277/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1135 - accuracy: 0.8497\n",
      "Epoch 278/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1141 - accuracy: 0.8497\n",
      "Epoch 279/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1144 - accuracy: 0.8511\n",
      "Epoch 280/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1136 - accuracy: 0.8539\n",
      "Epoch 281/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1139 - accuracy: 0.8483\n",
      "Epoch 282/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1131 - accuracy: 0.8497\n",
      "Epoch 283/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1132 - accuracy: 0.8511\n",
      "Epoch 284/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1133 - accuracy: 0.8497\n",
      "Epoch 285/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1131 - accuracy: 0.8497\n",
      "Epoch 286/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1132 - accuracy: 0.8525\n",
      "Epoch 287/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1130 - accuracy: 0.8483\n",
      "Epoch 288/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1136 - accuracy: 0.8497\n",
      "Epoch 289/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1133 - accuracy: 0.8469\n",
      "Epoch 290/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1135 - accuracy: 0.8525\n",
      "Epoch 291/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1139 - accuracy: 0.8469\n",
      "Epoch 292/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1133 - accuracy: 0.8525\n",
      "Epoch 293/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1133 - accuracy: 0.8511\n",
      "Epoch 294/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1136 - accuracy: 0.8525\n",
      "Epoch 295/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1160 - accuracy: 0.8483\n",
      "Epoch 296/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1140 - accuracy: 0.8539\n",
      "Epoch 297/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1129 - accuracy: 0.8567\n",
      "Epoch 298/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1128 - accuracy: 0.8497\n",
      "Epoch 299/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1133 - accuracy: 0.8525\n",
      "Epoch 300/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1129 - accuracy: 0.8581\n",
      "Epoch 301/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1130 - accuracy: 0.8483\n",
      "Epoch 302/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1124 - accuracy: 0.8525\n",
      "Epoch 303/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1128 - accuracy: 0.8497\n",
      "Epoch 304/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1127 - accuracy: 0.8553\n",
      "Epoch 305/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1142 - accuracy: 0.8525\n",
      "Epoch 306/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1131 - accuracy: 0.8469\n",
      "Epoch 307/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1126 - accuracy: 0.8525\n",
      "Epoch 308/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1135 - accuracy: 0.8539\n",
      "Epoch 309/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1134 - accuracy: 0.8539\n",
      "Epoch 310/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1119 - accuracy: 0.8581\n",
      "Epoch 311/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1134 - accuracy: 0.8497\n",
      "Epoch 312/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1135 - accuracy: 0.8511\n",
      "Epoch 313/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1123 - accuracy: 0.8539\n",
      "Epoch 314/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1136 - accuracy: 0.8525\n",
      "Epoch 315/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1122 - accuracy: 0.8539\n",
      "Epoch 316/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1121 - accuracy: 0.8525\n",
      "Epoch 317/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1130 - accuracy: 0.8525\n",
      "Epoch 318/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1125 - accuracy: 0.8511\n",
      "Epoch 319/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1120 - accuracy: 0.8567\n",
      "Epoch 320/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1130 - accuracy: 0.8539\n",
      "Epoch 321/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1121 - accuracy: 0.8511\n",
      "Epoch 322/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1124 - accuracy: 0.8525\n",
      "Epoch 323/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1124 - accuracy: 0.8567\n",
      "Epoch 324/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1123 - accuracy: 0.8553\n",
      "Epoch 325/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1122 - accuracy: 0.8539\n",
      "Epoch 326/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1148 - accuracy: 0.8455\n",
      "Epoch 327/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1160 - accuracy: 0.8469\n",
      "Epoch 328/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1124 - accuracy: 0.8539\n",
      "Epoch 329/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1126 - accuracy: 0.8497\n",
      "Epoch 330/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1128 - accuracy: 0.8539\n",
      "Epoch 331/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1135 - accuracy: 0.8455\n",
      "Epoch 332/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1119 - accuracy: 0.8525\n",
      "Epoch 333/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1116 - accuracy: 0.8539\n",
      "Epoch 334/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1125 - accuracy: 0.8525\n",
      "Epoch 335/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1127 - accuracy: 0.8497\n",
      "Epoch 336/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1123 - accuracy: 0.8525\n",
      "Epoch 337/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1122 - accuracy: 0.8553\n",
      "Epoch 338/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1120 - accuracy: 0.8581\n",
      "Epoch 339/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1123 - accuracy: 0.8567\n",
      "Epoch 340/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1116 - accuracy: 0.8539\n",
      "Epoch 341/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1114 - accuracy: 0.8483\n",
      "Epoch 342/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1116 - accuracy: 0.8553\n",
      "Epoch 343/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1120 - accuracy: 0.8497\n",
      "Epoch 344/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1115 - accuracy: 0.8525\n",
      "Epoch 345/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1117 - accuracy: 0.8539\n",
      "Epoch 346/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1121 - accuracy: 0.8511\n",
      "Epoch 347/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1110 - accuracy: 0.8596\n",
      "Epoch 348/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1117 - accuracy: 0.8539\n",
      "Epoch 349/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1112 - accuracy: 0.8525\n",
      "Epoch 350/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1113 - accuracy: 0.8567\n",
      "Epoch 351/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1110 - accuracy: 0.8581\n",
      "Epoch 352/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1122 - accuracy: 0.8539\n",
      "Epoch 353/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1119 - accuracy: 0.8511\n",
      "Epoch 354/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1112 - accuracy: 0.8525\n",
      "Epoch 355/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1108 - accuracy: 0.8525\n",
      "Epoch 356/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1108 - accuracy: 0.8539\n",
      "Epoch 357/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1114 - accuracy: 0.8553\n",
      "Epoch 358/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1111 - accuracy: 0.8581\n",
      "Epoch 359/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1116 - accuracy: 0.8511\n",
      "Epoch 360/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1106 - accuracy: 0.8511\n",
      "Epoch 361/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1112 - accuracy: 0.8553\n",
      "Epoch 362/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1113 - accuracy: 0.8511\n",
      "Epoch 363/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1115 - accuracy: 0.8525\n",
      "Epoch 364/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1109 - accuracy: 0.8525\n",
      "Epoch 365/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1110 - accuracy: 0.8567\n",
      "Epoch 366/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1113 - accuracy: 0.8581\n",
      "Epoch 367/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1106 - accuracy: 0.8525\n",
      "Epoch 368/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1111 - accuracy: 0.8539\n",
      "Epoch 369/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1119 - accuracy: 0.8596\n",
      "Epoch 370/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1102 - accuracy: 0.8525\n",
      "Epoch 371/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1119 - accuracy: 0.8511\n",
      "Epoch 372/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1105 - accuracy: 0.8525\n",
      "Epoch 373/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1107 - accuracy: 0.8581\n",
      "Epoch 374/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1104 - accuracy: 0.8553\n",
      "Epoch 375/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1107 - accuracy: 0.8596\n",
      "Epoch 376/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1111 - accuracy: 0.8525\n",
      "Epoch 377/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1105 - accuracy: 0.8539\n",
      "Epoch 378/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1109 - accuracy: 0.8525\n",
      "Epoch 379/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1117 - accuracy: 0.8539\n",
      "Epoch 380/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1106 - accuracy: 0.8581\n",
      "Epoch 381/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1112 - accuracy: 0.8511\n",
      "Epoch 382/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1120 - accuracy: 0.8525\n",
      "Epoch 383/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1111 - accuracy: 0.8525\n",
      "Epoch 384/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1100 - accuracy: 0.8539\n",
      "Epoch 385/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1108 - accuracy: 0.8553\n",
      "Epoch 386/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1104 - accuracy: 0.8539\n",
      "Epoch 387/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1099 - accuracy: 0.8610\n",
      "Epoch 388/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1103 - accuracy: 0.8553\n",
      "Epoch 389/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1103 - accuracy: 0.8553\n",
      "Epoch 390/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1108 - accuracy: 0.8553\n",
      "Epoch 391/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1103 - accuracy: 0.8610\n",
      "Epoch 392/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1108 - accuracy: 0.8553\n",
      "Epoch 393/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1099 - accuracy: 0.8553\n",
      "Epoch 394/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1103 - accuracy: 0.8539\n",
      "Epoch 395/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1106 - accuracy: 0.8553\n",
      "Epoch 396/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1114 - accuracy: 0.8525\n",
      "Epoch 397/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1113 - accuracy: 0.8497\n",
      "Epoch 398/500\n",
      "712/712 [==============================] - 0s 37us/sample - loss: 0.1108 - accuracy: 0.8596\n",
      "Epoch 399/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1103 - accuracy: 0.8567\n",
      "Epoch 400/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1106 - accuracy: 0.8525\n",
      "Epoch 401/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1104 - accuracy: 0.8539\n",
      "Epoch 402/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1112 - accuracy: 0.8511\n",
      "Epoch 403/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1107 - accuracy: 0.8539\n",
      "Epoch 404/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1118 - accuracy: 0.8525\n",
      "Epoch 405/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1106 - accuracy: 0.8553\n",
      "Epoch 406/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1094 - accuracy: 0.8567\n",
      "Epoch 407/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1099 - accuracy: 0.8567\n",
      "Epoch 408/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1094 - accuracy: 0.8553\n",
      "Epoch 409/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1106 - accuracy: 0.8553\n",
      "Epoch 410/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1100 - accuracy: 0.8567\n",
      "Epoch 411/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1093 - accuracy: 0.8581\n",
      "Epoch 412/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1092 - accuracy: 0.8596\n",
      "Epoch 413/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1102 - accuracy: 0.8596\n",
      "Epoch 414/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1097 - accuracy: 0.8539\n",
      "Epoch 415/500\n",
      "712/712 [==============================] - 0s 34us/sample - loss: 0.1095 - accuracy: 0.8567\n",
      "Epoch 416/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1094 - accuracy: 0.8553\n",
      "Epoch 417/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1106 - accuracy: 0.8553\n",
      "Epoch 418/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1092 - accuracy: 0.8610\n",
      "Epoch 419/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1101 - accuracy: 0.8581\n",
      "Epoch 420/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1090 - accuracy: 0.8511\n",
      "Epoch 421/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1092 - accuracy: 0.8581\n",
      "Epoch 422/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1093 - accuracy: 0.8553\n",
      "Epoch 423/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1091 - accuracy: 0.8596\n",
      "Epoch 424/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1090 - accuracy: 0.8553\n",
      "Epoch 425/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1100 - accuracy: 0.8553\n",
      "Epoch 426/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1090 - accuracy: 0.8596\n",
      "Epoch 427/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1087 - accuracy: 0.8596\n",
      "Epoch 428/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1092 - accuracy: 0.8596\n",
      "Epoch 429/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1089 - accuracy: 0.8624\n",
      "Epoch 430/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1098 - accuracy: 0.8553\n",
      "Epoch 431/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1098 - accuracy: 0.8553\n",
      "Epoch 432/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1104 - accuracy: 0.8539\n",
      "Epoch 433/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1088 - accuracy: 0.8567\n",
      "Epoch 434/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1083 - accuracy: 0.8596\n",
      "Epoch 435/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1106 - accuracy: 0.8553\n",
      "Epoch 436/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1119 - accuracy: 0.8539\n",
      "Epoch 437/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1092 - accuracy: 0.8525\n",
      "Epoch 438/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1088 - accuracy: 0.8596\n",
      "Epoch 439/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1087 - accuracy: 0.8525\n",
      "Epoch 440/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1088 - accuracy: 0.8553\n",
      "Epoch 441/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1086 - accuracy: 0.8553\n",
      "Epoch 442/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1088 - accuracy: 0.8581\n",
      "Epoch 443/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1087 - accuracy: 0.8596\n",
      "Epoch 444/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1080 - accuracy: 0.8581\n",
      "Epoch 445/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1089 - accuracy: 0.8567\n",
      "Epoch 446/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1079 - accuracy: 0.8581\n",
      "Epoch 447/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1088 - accuracy: 0.8581\n",
      "Epoch 448/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1086 - accuracy: 0.8567\n",
      "Epoch 449/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1083 - accuracy: 0.8567\n",
      "Epoch 450/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1079 - accuracy: 0.8553\n",
      "Epoch 451/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1086 - accuracy: 0.8567\n",
      "Epoch 452/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1103 - accuracy: 0.8539\n",
      "Epoch 453/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1084 - accuracy: 0.8581\n",
      "Epoch 454/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1083 - accuracy: 0.8567\n",
      "Epoch 455/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1077 - accuracy: 0.8567\n",
      "Epoch 456/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1080 - accuracy: 0.8511\n",
      "Epoch 457/500\n",
      "712/712 [==============================] - 0s 24us/sample - loss: 0.1078 - accuracy: 0.8567\n",
      "Epoch 458/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1084 - accuracy: 0.8539\n",
      "Epoch 459/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1088 - accuracy: 0.8553\n",
      "Epoch 460/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1083 - accuracy: 0.8553\n",
      "Epoch 461/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1087 - accuracy: 0.8567\n",
      "Epoch 462/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1078 - accuracy: 0.8624\n",
      "Epoch 463/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1084 - accuracy: 0.8567\n",
      "Epoch 464/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1080 - accuracy: 0.8596\n",
      "Epoch 465/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1074 - accuracy: 0.8567\n",
      "Epoch 466/500\n",
      "712/712 [==============================] - 0s 22us/sample - loss: 0.1095 - accuracy: 0.8610\n",
      "Epoch 467/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1080 - accuracy: 0.8581\n",
      "Epoch 468/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1077 - accuracy: 0.8553\n",
      "Epoch 469/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1086 - accuracy: 0.8610\n",
      "Epoch 470/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1074 - accuracy: 0.8596\n",
      "Epoch 471/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1074 - accuracy: 0.8553\n",
      "Epoch 472/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1090 - accuracy: 0.8553\n",
      "Epoch 473/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1076 - accuracy: 0.8553\n",
      "Epoch 474/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1079 - accuracy: 0.8553\n",
      "Epoch 475/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1078 - accuracy: 0.8539\n",
      "Epoch 476/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1084 - accuracy: 0.8596\n",
      "Epoch 477/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1082 - accuracy: 0.8539\n",
      "Epoch 478/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1099 - accuracy: 0.8596\n",
      "Epoch 479/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1078 - accuracy: 0.8652\n",
      "Epoch 480/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1076 - accuracy: 0.8610\n",
      "Epoch 481/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1075 - accuracy: 0.8638\n",
      "Epoch 482/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1071 - accuracy: 0.8596\n",
      "Epoch 483/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1091 - accuracy: 0.8596\n",
      "Epoch 484/500\n",
      "712/712 [==============================] - 0s 35us/sample - loss: 0.1082 - accuracy: 0.8567\n",
      "Epoch 485/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1082 - accuracy: 0.8610\n",
      "Epoch 486/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1071 - accuracy: 0.8610\n",
      "Epoch 487/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1071 - accuracy: 0.8539\n",
      "Epoch 488/500\n",
      "712/712 [==============================] - 0s 32us/sample - loss: 0.1070 - accuracy: 0.8610\n",
      "Epoch 489/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1085 - accuracy: 0.8581\n",
      "Epoch 490/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1071 - accuracy: 0.8567\n",
      "Epoch 491/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1075 - accuracy: 0.8596\n",
      "Epoch 492/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1070 - accuracy: 0.8553\n",
      "Epoch 493/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1068 - accuracy: 0.8567\n",
      "Epoch 494/500\n",
      "712/712 [==============================] - 0s 29us/sample - loss: 0.1074 - accuracy: 0.8596\n",
      "Epoch 495/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1065 - accuracy: 0.8596\n",
      "Epoch 496/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1068 - accuracy: 0.8624\n",
      "Epoch 497/500\n",
      "712/712 [==============================] - 0s 25us/sample - loss: 0.1065 - accuracy: 0.8553\n",
      "Epoch 498/500\n",
      "712/712 [==============================] - 0s 27us/sample - loss: 0.1064 - accuracy: 0.8553\n",
      "Epoch 499/500\n",
      "712/712 [==============================] - 0s 28us/sample - loss: 0.1062 - accuracy: 0.8539\n",
      "Epoch 500/500\n",
      "712/712 [==============================] - 0s 31us/sample - loss: 0.1074 - accuracy: 0.8596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20a84a802e8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanicModel.fit(x = x_train_keras, y = y_train_keras, epochs = 500, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 0s 285us/sample - loss: 0.1224 - accuracy: 0.8268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12235137245841532, 0.82681566]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanicModel.evaluate(x = x_test_keras, y= y_test_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
